# ğŸ§  MCP Server (Model Compute Paradigm)

A modular, production-ready FastAPI server built to route and orchestrate multiple AI/LLM-powered models behind a unified, scalable interface. It supports **streaming chat**, **LLM-based routing**, and **multi-model pipelines** (like analyze â†’ summarize â†’ recommend) â€“ all asynchronously and fully Dockerized.

---
## ğŸ¯ Project Score (Production Readiness)

| Capability                      | Status     | Details                                                                      |
|-------------------------------|------------|-------------------------------------------------------------------------------|
| ğŸ§  Multi-Model Orchestration   | âœ… Complete | Dynamic routing between `chat`, `summarize`, `sentiment`, `recommend`        |
| ğŸ¤– LLM-Based Task Router       | âœ… Complete | GPT-powered routing via `"auto"` task type                                   |
| ğŸ” Async FastAPI + Concurrency | âœ… Complete | Async/await + concurrent task execution with simulated/model API delays      |
| ğŸ”Š GPT Streaming Support       | âœ… Complete | `text/event-stream` chunked responses for chat endpoints                     |
| ğŸ§ª Unit + Mocked API Tests     | âœ… Complete | Pytest-based test suite with mocked `run()` responses                        |
| ğŸ³ Dockerized + Clean Layout   | âœ… Complete | Python 3.13 base image, no Conda dependency, production-ready Dockerfile     |
| ğŸ“¦ Metadata-Driven Registry    | âœ… Complete | Model metadata loaded from external YAML config                              |
| ğŸ” Rate Limiting & Retry       | â³ In Progress | Handles 429 retry loop; rate limiting controls WIP                        |
| ğŸ§ª CI + Docs                   | â³ Next     | GitHub Actions + Swagger/Redoc planned                                       |

---
## ğŸ§© Why This Project? (Motivation)

Modern ML/LLM deployments often involve:
- Multiple task types and model backends (OpenAI, HF, local, REST)
- Routing decisions based on input intent
- Combining outputs of multiple models (e.g., `summarize` + `recommend`)
- Handling 429 retries, async concurrency, streaming responses

ğŸ”§ However, building such an **LLM backend API server** that is:
- Async + concurrent
- Streamable
- Pluggable (via metadata)
- Testable
- Dockerized
â€¦ is **non-trivial** and not easily found in one single place.

---
## ğŸ’¡ What Weâ€™ve Built (Solution)

This repo is a **production-ready PoC** of an MCP (Model-Compute Paradigm) architecture:

- âœ… **FastAPI-based microserver** to handle multiple tasks via `/task` endpoint
- âœ… Task router that can:
  - ğŸ” Dispatch to specific model types (`chat`, `sentiment`, `summarize`, `recommend`)
  - ğŸ¤– Use an LLM to infer which task to run (`auto`)
  - ğŸ§  Run multiple models in sequence (`analyze`)
- âœ… GPT streaming via `text/event-stream`
- âœ… Async/await enabled architecture for concurrency
- âœ… Clean modular code for easy extension
- âœ… Dockerized for deployment
- âœ… Tested using Pytest with mocking

---

## ğŸ› ï¸ Use Cases

| Use Case                                | MCP Server Support                           |
|----------------------------------------|-----------------------------------------------|
| Build your own ChatGPT-style API       | âœ… `chat` task with streaming                  |
| Build intelligent task router           | âœ… `auto` task with GPT-powered intent parsing |
| Build AI pipelines (like RAG/RL)        | âœ… `analyze` task with sequential execution    |
| Swap between OpenAI/HuggingFace APIs   | âœ… Via `model_registry.yaml` config           |
| Add custom models (e.g., OCR, vision)   | âœ… Just add a new module + registry entry     |

---


## ğŸš€ Features

- âœ… **Async FastAPI** server
- ğŸ§  **Task-based Model Routing** (`chat`, `sentiment`, `recommender`, `summarize`)
- ğŸ“„ **Model Registry** from YAML/JSON
- ğŸ” **Automatic Retry** and **Rate Limit Handling** for APIs
- ğŸ”„ **Streaming Responses** for Chat
- ğŸ§ª **Unit Tests + Mocked API Calls**
- ğŸ³ **Dockerized** for production deployment
- ğŸ“¦ Modular structure, ready for CI/CD

---

## ğŸ— Architecture Overview

```plaintext
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  Frontend  â”‚
               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        YAML/JSON
              â”‚  FastAPI   â”‚â—„â”€â”€â”€â”€â” Model Registry
              â”‚   Server   â”‚     â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚              â”‚              â”‚
     â–¼              â–¼              â–¼
 [chat]         [sentiment]   [recommender]
  GPT-4         HF pipeline   stub logic / API

---
ğŸ›  Setup
ğŸ“¦ Install dependencies
git clone https://github.com/YOUR_USERNAME/mcp-server.git
cd mcp-server
---
# Optional: create virtualenv
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
or
conda create -n <env_name>
conda activate <env_name>

pip install -r requirements.txt

â–¶ï¸ Run the server
uvicorn app:app --reload

Access the docs at: http://localhost:8000/docs


ğŸ§ª Running Tests
pytest tests/

Unit tests mock external API calls using unittest.mock.AsyncMock.

ğŸ³ Docker Support
ğŸ”¨ Build image
docker build -t mcp-server .

ğŸš€ Run container
docker run -p 8000:8000 mcp-server

ğŸ§° Example API Request
curl -X POST http://localhost:8000/task \
  -H "Content-Type: application/json" \
  -d '{
        "type": "chat",
        "input": "What are the benefits of restorative yoga?"
      }'

ğŸ” Directory Structure
mcp/
â”œâ”€â”€ app.py                  # FastAPI entry
â”œâ”€â”€ models/                 # ML models (chat, sentiment, etc.)
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ task_router.py      # Task router
â”‚   â””â”€â”€ model_registry.py   # Registry loader
â”œâ”€â”€ registry/models.yaml    # YAML registry of model metadata
â”œâ”€â”€ tests/                  # Unit tests
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env / .gitignore


ğŸ¤ Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what youâ€™d like to change.

ğŸ“„ License
MIT

âœ¨ Author
Built by Sriram Kumar Reddy Challa
